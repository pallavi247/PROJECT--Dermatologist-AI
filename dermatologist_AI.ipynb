{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dermatologist_AI.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSPLlP9sijSr7biIFBC0Gq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xOUWXxy_dKZ9"},"source":["# Dermatologist AI To Detect Skin Cancer\n","\n","Skin Cancer is the most common cancer in the world. In U.S., there are 5.4 million new cases every year. They come in different types. Some are called Carcinomas and some are called Melanomas. Melanomas are the ones that typically kill people, its called the Black Cancer. 20% of Americans eventually get skin cancer. In most cases benign. Pre-cancer also called Actinic Kertosis, affects 58 miliion Americans and many more in the world. In this country there are 76,000 Melanomas each year and 10,000 deaths. Skin cancers comes in multiple stages from stage zero to stage four. \n","\n","According to the American Cancer Society, the 5-year survival rate for stage 4 melanoma is 15–20 percent. This means that an estimated 15–20 percent of people with stage 4 melanoma will be alive 5 years after diagnosis. But the 5-year survival rate for Stage 0, is 98.4%. Therefore the early detection is paramount to not dying from skin cancer.\n","\n","Dermatologists are extremely highly trained to find melanomas and carcinomas, because they need to make life and death decisions in diagnosing patient's skin. But they are very few in numbers and the early detection is still a problem. If human intelligence is also aided with an highly trained AI deep neural networks, it can revolutionize and save the world, by early detection.\n"]},{"cell_type":"markdown","metadata":{"id":"1vDZIc_ndVWu"},"source":["## Goal\n","Skin cancer is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories [[ 1 ] ](https://www.nature.com/articles/nature21056.epdf?author_access_token=8oxIcYWf5UNrNpHsUHd2StRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPakXos4UhQAFZ750CsBNMMsISFHIKinKDMKjShCpHIlYPYUHhNzkn6pSnOCt0Ftf6)\n","\n","The goal of this project is to design an algorithm that can visually diagnose melanoma. In particular, the algorithm will distinguish this malignant skin tumor from two types of benign lesions (nevi and seborrheic keratoses)."]},{"cell_type":"markdown","metadata":{"id":"j2QWQozZdbIv"},"source":["## Roadmap\n","\n","The notebook is broken into below steps:\n","\n","* Step 0: Setup Google Colab\n","* Step 1: Imports and CUDA check\n","* Step 2: Load and Transform Data\n","* Step 3: Define Model\n","* Step 4: Final Classifier Layer\n","* Step 5: Training\n","* Step 6: Testing\n","* Step 7: Visualize Sample Test Results\n"]},{"cell_type":"markdown","metadata":{"id":"Sl9P7Lem60aw"},"source":["## Set up Google Colab"]},{"cell_type":"code","metadata":{"id":"J4agtmXX65wc","executionInfo":{"status":"ok","timestamp":1603025659103,"user_tz":-120,"elapsed":31375,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"2723bf71-8670-4e70-94ac-03d36bf04d0d","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a0f-348EXraB"},"source":["# Check if PIL works\n","from PIL import Image\n","Image.open(\"/content/drive/My Drive/Colab Notebooks/dermatologist_AI/data/train/melanoma/ISIC_0000002.jpg\").convert('RGB')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVt2FGfsZZ6n"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"JnVo7YOsZcrq","executionInfo":{"status":"ok","timestamp":1603025685705,"user_tz":-120,"elapsed":5434,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}}},"source":["import numpy as np\n","import torch\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import os\n","\n","%matplotlib inline"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_SjSxKekgsC","executionInfo":{"status":"ok","timestamp":1603025695595,"user_tz":-120,"elapsed":1767,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"195d2af4-18cd-452b-9665-eb01b0d3ebd5","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ukoD8O8tqKBR"},"source":["\n","\n","> ## Load and Transform Data\n","\n"]},{"cell_type":"code","metadata":{"id":"DeLrlQvyqIOu","executionInfo":{"status":"ok","timestamp":1603025700998,"user_tz":-120,"elapsed":1367,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}}},"source":["# define training and test data directories\n","data_dir = '/content/drive/My Drive/Colab Notebooks/dermatologist_AI/data/'\n","train_dir = 'train/'\n","valid_dir = 'valid/'\n","test_dir = 'test/'\n","\n","# classes are folders in each directory with these names\n","classes = ['melanoma', 'nevus', 'seborrheic_keratosis']"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RaPVcN0AAdgF"},"source":["## Transform Data"]},{"cell_type":"code","metadata":{"id":"JhfBsnfuvN9o","executionInfo":{"status":"ok","timestamp":1603025707934,"user_tz":-120,"elapsed":3999,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}}},"source":["# Load and transform data using Image Folder\n","batch_size = 20\n","num_workers = 0\n","#Inception-v3 299x299 images as input, so we resize all of them\n","transform = transforms.Compose([transforms.Resize(320),\n","                                transforms.CenterCrop(299),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                                                     std = [0.229, 0.224, 0.225])])\n","\n","train_data = datasets.ImageFolder(data_dir + train_dir,transform=transform )\n","#train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n","valid_data = datasets.ImageFolder(data_dir + valid_dir, transform=transform)\n","test_data = datasets.ImageFolder(data_dir + test_dir, transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)\n","\n","loaders = {'train' : train_loader,\n","           'valid' : valid_loader,\n","           'test' : test_loader}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWWfz1nV_6cV","executionInfo":{"status":"ok","timestamp":1603009359218,"user_tz":-120,"elapsed":856,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"3b76cb4d-7736-47ba-fd22-8c50e5e952cf","colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["class_names = train_data.classes\n","nb_classes = len(class_names)\n","\n","print(\"Number of classes:\", nb_classes)\n","print(\"\\nClass names: \\n\\n\", class_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of classes: 3\n","\n","Class names: \n","\n"," ['melanoma', 'nevus', 'seborrheic_keratosis']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-fqAWj68AUPO"},"source":["## Data Visualization"]},{"cell_type":"code","metadata":{"id":"rAzQcmuBAk11"},"source":["# Visualize some sample data\n","inputs, classes = next(iter(train_loader))\n","\n","for image, label in zip(inputs, classes): \n","    image = image.to(\"cpu\").clone().detach()\n","    image = image.numpy().squeeze()\n","    image = image.transpose(1,2,0)\n","    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n","    image = image.clip(0, 1)     \n","    fig = plt.figure(figsize=(12,3))\n","    plt.imshow(image)\n","    plt.title(class_names[label])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32-D6ma3Mns9"},"source":["## Model Definition\n","Here I've used a pretrained model Inception_v3.\n","Also called GoogleNetv3, a famous ConvNet trained on Imagenet from 2015\n"]},{"cell_type":"code","metadata":{"id":"y9P3V_g2NXOE"},"source":["model = models.inception_v3(pretrained = True)\n","#print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFEIat1wWtqb","executionInfo":{"status":"ok","timestamp":1603025729022,"user_tz":-120,"elapsed":1024,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}}},"source":["# Freeze training for all conv layers\n","for param in model.parameters():\n","  param.requires_grad = False"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRiFvclRYzwe"},"source":["## Final Classifier Layer"]},{"cell_type":"code","metadata":{"id":"olzQ3rE5XoCN","executionInfo":{"status":"ok","timestamp":1603025763581,"user_tz":-120,"elapsed":11360,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}}},"source":["import torch.nn as nn\n","from collections import OrderedDict\n","\n","# add last linear layer (n_inputs -> 133 dog's breed classes)\n","n_output = 3\n","n_input = model.fc.in_features\n","\n","classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(2048, 512)),\n","                                         ('relu', nn.ReLU()),\n","                                         ('drop', nn.Dropout(0.5)),\n","                                         ('fc2', nn.Linear(512, 3)), \n","                                         ('output', nn.Softmax(dim=1))]))\n","model.fc = classifier\n","#print(model)\n","\n","if train_on_gpu:\n","    model = model.cuda()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"auvoMviQ3dQg","executionInfo":{"status":"ok","timestamp":1603025768209,"user_tz":-120,"elapsed":995,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}}},"source":["import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.fc.parameters(), lr=0.011)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PkRnKn4G4BCu"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"L25ZwmlxDdgj"},"source":["def accuracy(validloader):    \n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_size, (data,target) in enumerate(valid_loader):\n","          if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","    \n","          # forward pass\n","          output, _ = model(data)\n","          # calculate teh batch loss\n","          _, predicted = torch.max(output.data, 1)\n","          total += target.size(0)\n","          correct += (predicted == target).sum().item()\n","\n","        print('Accuracy of the network %d %%' % (100 * correct / total))\n","\n","\n","accuracy(valid_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZ_FzsZLnsHt","executionInfo":{"status":"ok","timestamp":1603024787861,"user_tz":-120,"elapsed":847,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"11b942f0-59cf-4efc-c608-af290782b61f","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["  if model is not None:\n","    print(\"in if\")\n","    del model\n","    torch.cuda.empty_cache()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["in if\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sYkogOoV4Arf","executionInfo":{"status":"error","timestamp":1603024782514,"user_tz":-120,"elapsed":1642314,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"724f11e4-cf56-44e9-994d-1d579e8b17ce","colab":{"base_uri":"https://localhost:8080/","height":415}},"source":["n_epochs = 10\n","# initialize tracker for minimum validation loss\n","valid_loss_min = np.Inf \n","for epoch in range(1, n_epochs+1):\n","  train_loss = 0.0\n","  valid_loss = 0.0\n","\n","  ###################\n","  # train the model #\n","  ###################\n","  model.train() # prep model for training\n","  for batch_size, (data,target) in enumerate(train_loader):    \n","    # move tensors to GPU if CUDA is available\n","    if train_on_gpu:\n","      data, target = data.cuda(), target.cuda()\n","    #clear the gradients of all optimized variables\n","    optimizer.zero_grad()\n","    # forward pass\n","    output,_ = model(data)\n","    # calculate teh batch loss\n","    loss = criterion(output, target)\n","    # backward pass\n","    loss.backward()\n","    # optimization step\n","    optimizer.step()\n","    # update training loss\n","    train_loss += loss.item() * data.size(0)\n","\n","  ###################\n","  # train the model #\n","  ###################\n","  model.eval() # prep model for evaluation\n","  for batch_size, (data,target) in enumerate(valid_loader):\n","    if train_on_gpu:\n","      data, target = data.cuda(), target.cuda()\n","    \n","    # forward pass\n","    output = model(data)\n","    # calculate teh batch loss\n","    loss = criterion(output, target)\n","    #loss = criterion(output, target)\n","    valid_loss += loss.item()*data.size(0)\n","  # print training/validation statistics \n","  # calculate average loss over an epoch\n","  train_loss = train_loss/len(train_loader.sampler)\n","  valid_loss = valid_loss/len(valid_loader.sampler)\n","\n","  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","      epoch+1, \n","      train_loss,\n","      valid_loss\n","      ))\n","    \n","  # save model if validation loss has decreased\n","  if valid_loss <= valid_loss_min:\n","    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","    torch.save(model.state_dict(), \"./model.pth\")\n","    valid_loss_min = valid_loss\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 2 \tTraining Loss: 0.925408 \tValidation Loss: 1.029301\n","Validation loss decreased (inf --> 1.029301).  Saving model ...\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-fbd1f762d958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# calculate teh batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"gBtFi2YQ1578","executionInfo":{"status":"ok","timestamp":1603018385204,"user_tz":-120,"elapsed":1699,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"31dd9de5-3c5f-479b-d8a8-f55ff4ef58f6","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# load the model that got the best validation accuracy\n","model.load_state_dict(torch.load('./model.pth'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"QsvnbF2wwF4D","executionInfo":{"status":"ok","timestamp":1603019058487,"user_tz":-120,"elapsed":669820,"user":{"displayName":"Pallavi Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bpuY3dCNXM8l2dzMVf7BYpwSL-zWEBUxbE-4rg=s64","userId":"16141281139754075931"}},"outputId":"08ef4cd4-1d31-4882-f247-67ec999a35ab","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# track test loss \n","# over 3 classes\n","test_loss = 0.0\n","class_correct = list(0. for i in range(5))\n","class_total = list(0. for i in range(5))\n","\n","model.eval() # eval mode\n","\n","# iterate over test data\n","for data, target in test_loader:\n","    # move tensors to GPU if CUDA is available\n","    if train_on_gpu:\n","        data, target = data.cuda(), target.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the batch loss\n","    loss = criterion(output, target)\n","    # update  test loss \n","    test_loss += loss.item()*data.size(0)\n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    # calculate test accuracy for each object class\n","    for i in range(batch_size):\n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# calculate avg test loss\n","test_loss = test_loss/len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(3):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","            class_names[i], 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (class_names[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss: 1.044302\n","\n","Test Accuracy of melanoma:  0% ( 0/20)\n","Test Accuracy of nevus: 100% (45/45)\n","Test Accuracy of seborrheic_keratosis:  0% ( 0/19)\n","\n","Test Accuracy (Overall): 53% (45/84)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UYieNkugxhK6"},"source":["# obtain one batch of test images\n","dataiter = iter(test_loader)\n","images, labels = dataiter.next()\n","images.numpy()\n","\n","# move model inputs to cuda, if GPU available\n","if train_on_gpu:\n","    images = images.cuda()\n","\n","# get sample outputs\n","output = model(images)\n","# convert output probabilities to predicted class\n","_, preds_tensor = torch.max(output, 1)\n","preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n","\n","# plot the images in the batch, along with predicted and true labels\n","fig = plt.figure(figsize=(25, 4))\n","for idx in np.arange(20):\n","    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n","    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n","    ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n","                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"],"execution_count":null,"outputs":[]}]}